{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48015ee0-9b0e-4d33-ab06-842d8791012a",
   "metadata": {},
   "source": [
    "# In-Class Challenge Assignment: Experimenting with the Perceptron\n",
    "# Day 19 Extension\n",
    "# CMSE 202"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87657ab-983d-4a2e-983d-4c3ef9847d08",
   "metadata": {},
   "source": [
    "## Now that you have a working Perceptron Classifier... let's experiment with it a bit!\n",
    "\n",
    "When building and testing your Perceptron Classifier you used a simplified version of the iris dataset that has been reduced to just two features and two class labels. \n",
    "\n",
    "### Will your Perceptron classifier work on a more complex dataset?\n",
    "\n",
    "Another widely used dataset for experimenting with binary classification is the [sonar dataset](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)).\n",
    "\n",
    "A version of this dataset can be found here:\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/sonar.csv`\n",
    "\n",
    "Make sure you take a moment to read the [UC Irvine Machine Learning Repository page](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)) to understand exactly what is in this dataset, but essentially is a collection of sonar measurements of rocks and \"mines\" (metal cynlinders). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e2e8b-c372-4871-9862-5df2a65e9afa",
   "metadata": {},
   "source": [
    "---\n",
    "### Testing your new tool and exploring others\n",
    "\n",
    "With any time that you have left in class, see if you can accomplish the following:\n",
    "\n",
    "1. Load up the sonar dataset and change the class labels so that they can be used with the Perceptron classifier.\n",
    "\n",
    "2. Use the Perceptron classifier you built from scratch to see how well you can do at distinguishing rocks from mines. You may need to make some modifications to your code if you didn't build it to be flexible enough to accept an arbitary number of data deatures. Experiment with the learning rate and number of iterations to see how high of an accuracy you can get with your classifier.\n",
    "\n",
    "3. If you get your Perceptron classifier working, can you figure out how to use the Perceptron Classifier that is available in [scikit-learn](https://scikit-learn.org/stable/index.html)? You may need to do a bit of Google searching and exploration of the documentation to figure this out. How well does the scikit-learn version do compared to the one you built?\n",
    "\n",
    "<!--\n",
    "4. If you're feeling really ambitious, can you build a Perceptron classifier with [Tensorflow](https://www.tensorflow.org/)? Remember, the Perceptron is basically just a single-neuron single-layer neural network. This requires installation of Tensorflow and relevant APIs.\n",
    "-->\n",
    "\n",
    "4. The logistic regression model (Day 15) is also a multi-variable classifier. Use it on the same dataset. Compare the results of your perceptron classifier against that obtained from and discuss your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82c234-ce27-4854-bbfc-96629f3f3028",
   "metadata": {},
   "source": [
    "---\n",
    "&#9989; **Do This**: Load up the sonar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d795f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put code here to start and generate new cells as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3479b0d-1efe-4444-bbde-73c3498c116c",
   "metadata": {},
   "source": [
    "###ANSWER\n",
    "\n",
    "The code below just shows that this can be done as a reference when students work through this. If student head down the Tensorflow path, this article can be useful as a reference: https://www.quantstart.com/articles/training-the-perceptron-with-scikit-learn-and-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad27d63-0fee-41b5-80ad-cb0a1e99de99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qg/fd6rwmln2ys1_p2s6xdh0b800000gn/T/ipykernel_23939/540547923.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  sonar['Class'] = sonar['Class'].replace('Mine',-1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
       "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
       "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
       "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
       "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
       "\n",
       "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
       "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
       "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
       "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
       "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
       "\n",
       "     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "0          0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1          0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2          0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3          0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4          0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203        0.0116        0.0098        0.0199        0.0033        0.0101   \n",
       "204        0.0061        0.0093        0.0135        0.0063        0.0063   \n",
       "205        0.0160        0.0029        0.0051        0.0062        0.0089   \n",
       "206        0.0086        0.0046        0.0126        0.0036        0.0035   \n",
       "207        0.0146        0.0129        0.0047        0.0039        0.0061   \n",
       "\n",
       "     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n",
       "0          0.0180        0.0084        0.0090        0.0032      1  \n",
       "1          0.0140        0.0049        0.0052        0.0044      1  \n",
       "2          0.0316        0.0164        0.0095        0.0078      1  \n",
       "3          0.0050        0.0044        0.0040        0.0117      1  \n",
       "4          0.0072        0.0048        0.0107        0.0094      1  \n",
       "..            ...           ...           ...           ...    ...  \n",
       "203        0.0065        0.0115        0.0193        0.0157     -1  \n",
       "204        0.0034        0.0032        0.0062        0.0067     -1  \n",
       "205        0.0140        0.0138        0.0077        0.0031     -1  \n",
       "206        0.0034        0.0079        0.0036        0.0048     -1  \n",
       "207        0.0040        0.0036        0.0061        0.0115     -1  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###ANSWER\n",
    "import pandas as pd\n",
    "sonar = pd.read_csv(\"sonar.csv\")\n",
    "sonar['Class'] = sonar['Class'].replace('Rock',1)\n",
    "sonar['Class'] = sonar['Class'].replace('Mine',-1)\n",
    "sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a6ae6-4295-4455-baba-26a15b0ef359",
   "metadata": {},
   "source": [
    "Copy your percentron class to the cell below. **Note** sklearn has a **Perceptron** function. We should avoid using the same function name of your perceptron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf7ae4-71bf-4357-8fad-50b29631c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fb839fa-e796-4905-ae85-bf3fa00401e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANSWER\n",
    "import numpy as np\n",
    "\n",
    "class MyPerceptron():\n",
    "\n",
    "    def __init__ (self, labeled_data, iters, learning_rate):        \n",
    "        self.data = np.array(labeled_data)\n",
    "        self.weights = np.full((self.data.shape[1]), 1.0) # note that this is just an easy way to get \"number of features + 1\" and set them to 1.0\n",
    "        self.iters = iters\n",
    "        self.l_r = learning_rate\n",
    "        print(self.weights.shape)\n",
    "        \n",
    "    def predict(self, feature_set):\n",
    "        # print(\"Features: \",feature_set, \" weights: \", self.weights[1:])\n",
    "        res = np.dot(feature_set, self.weights[1:])\n",
    "        res += self.weights[0]\n",
    "        return (1 if res > 0 else -1)\n",
    "    \n",
    "    def fit(self):\n",
    "        for _ in range(self.iters):\n",
    "            for row in self.data:\n",
    "                update = self.l_r * (row[-1] - self.predict(row[:-1]))\n",
    "                self.weights[1:] += update * row[:-1]\n",
    "                self.weights[0] += update\n",
    "\n",
    "    def errors(self):\n",
    "        error_count = 0\n",
    "        for row in self.data:\n",
    "            # print(\"data: \", row[:-1], \"actual: \", row[-1], \"predict: \", self.predict(row[:-1]), \"weights: \", self.weights)\n",
    "            if (self.predict(row[:-1]) != row[-1]):\n",
    "                error_count += 1\n",
    "        print(\"Accuracy: \", 1.0-(error_count/len(self.data)))\n",
    "        print(\"Weights: \", self.weights[1:])\n",
    "        print(\"Bias weight:\", self.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23a7a6-c8fb-4f54-82bd-5c8afc031de4",
   "metadata": {},
   "source": [
    "Train your percentron class with sonar data. What's the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad22774-8b2e-44ea-ac54-67a0a94bf570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fe6863f-e06e-4426-9319-3a5b63be9cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61,)\n",
      "Accuracy:  0.5384615384615384\n",
      "Weights:  [ 0.246442  0.22178   0.030222 -1.00875  -0.305072  0.14447  -0.065726\n",
      "  0.709676  0.362512  0.218004 -0.432916  0.208082  0.21734  -0.21619\n",
      " -0.461566 -0.003092  0.214108 -0.530308  0.123584  0.285384 -0.63628\n",
      " -0.088484  0.75645  -0.34088  -0.162724  0.048546  0.276128 -0.296378\n",
      "  0.361932 -0.473136  0.247314  0.158088 -0.614624  0.39675  -0.269898\n",
      "  0.091126  0.29772   0.196604  0.18205   0.056166 -0.262448 -0.371116\n",
      " -0.389234  0.427424 -0.200874 -0.509154  0.110094 -0.22357   0.175514\n",
      "  0.925878  0.781926  0.767078  0.802206  0.758534  0.872354  0.949104\n",
      "  0.935354  0.71146   0.717388  0.740572]\n",
      "Bias weight: -0.3200000000000005\n"
     ]
    }
   ],
   "source": [
    "###ANSWER\n",
    "\n",
    "# print(sonar.values.shape[0])\n",
    "iters = sonar.values.shape[0]\n",
    "\n",
    "p = MyPerceptron(sonar.values,iters,0.01)\n",
    "p.fit()\n",
    "p.errors()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ceb7a-95db-49aa-81b6-ec094265ffdc",
   "metadata": {},
   "source": [
    "---\n",
    "&#9989; **Do This**: Use the **Perceptron** function from sklearn library to classify the same dataset in the cell below. Compare to your percentron classifier, how is the performance of the percetron in the sklearn library?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88194cf3-de9b-42d6-bdbb-d3f4ab4957bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "957f3433-2705-417c-8391-0d6c34e6c1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8365384615384616\n",
      "Predictions: [-1. -1. -1. -1. -1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1. -1. -1.  1. -1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "###ANSWER\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "model = Perceptron(max_iter=10,eta0=0.01)\n",
    "X = p.data[:,:-1]\n",
    "y = p.data[:,-1]\n",
    "model.fit(X,y)\n",
    "print(\"Accuracy:\",model.score(X,y))\n",
    "print(\"Predictions:\",model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702ab39-642d-413f-97ed-23f705e15611",
   "metadata": {},
   "source": [
    "---\n",
    "&#9989; **Do This**: Use **logistic regress model** from statsmodel library to classify the same dataset in the cell below. \n",
    "\n",
    "* Note that the full sonar data set contains some values that will result in singular values in the logistic regression. Thus, we will use only the first 40 attritbutes (columns) in the sonar dataset. The class label is still the last column in the sonar dataset.\n",
    "* We will add constant to the model, which is equivalent to the bias weight.\n",
    "* The Logit function requires the labels to be 1 or 0. You'll need to replace '-1' in the labels to '0' for the Logit function.\n",
    "* Let's set test_size = 0.15 in the train-test split, and fit the model using the training set.\n",
    "* Predict the labels of the test set. How is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65817556-9766-40ae-bcbb-96e12a04c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bc4d73b-90e0-418f-988e-c9462234b27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.260712\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  176\n",
      "Model:                          Logit   Df Residuals:                      135\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Tue, 01 Apr 2025   Pseudo R-squ.:                  0.6210\n",
      "Time:                        23:11:11   Log-Likelihood:                -45.885\n",
      "converged:                       True   LL-Null:                       -121.07\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.078e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.5924      3.085      1.489      0.137      -1.453      10.638\n",
      "x1          -104.0795     33.164     -3.138      0.002    -169.079     -39.080\n",
      "x2           -67.6984     25.675     -2.637      0.008    -118.021     -17.376\n",
      "x3            80.4905     25.056      3.212      0.001      31.381     129.600\n",
      "x4           -40.1401     16.417     -2.445      0.014     -72.318      -7.962\n",
      "x5            12.7456     13.003      0.980      0.327     -12.739      38.230\n",
      "x6           -23.8183     14.366     -1.658      0.097     -51.975       4.338\n",
      "x7            38.1504     14.190      2.689      0.007      10.338      65.962\n",
      "x8             9.7978      9.034      1.085      0.278      -7.909      27.504\n",
      "x9           -15.8998      9.342     -1.702      0.089     -34.211       2.411\n",
      "x10            0.3836      8.403      0.046      0.964     -16.086      16.853\n",
      "x11           -1.5095      8.745     -0.173      0.863     -18.650      15.631\n",
      "x12          -12.8284      7.803     -1.644      0.100     -28.123       2.466\n",
      "x13           -4.0167      7.114     -0.565      0.572     -17.960       9.927\n",
      "x14            1.1738      8.219      0.143      0.886     -14.935      17.283\n",
      "x15           -5.0287      7.279     -0.691      0.490     -19.295       9.238\n",
      "x16           13.7440      7.110      1.933      0.053      -0.192      27.680\n",
      "x17           -1.1177      6.014     -0.186      0.853     -12.904      10.669\n",
      "x18           -4.1764      6.049     -0.690      0.490     -16.033       7.680\n",
      "x19            0.9214      6.839      0.135      0.893     -12.483      14.326\n",
      "x20            1.9190      8.216      0.234      0.815     -14.185      18.023\n",
      "x21           -1.5884      7.840     -0.203      0.839     -16.954      13.777\n",
      "x22           -3.6346      7.006     -0.519      0.604     -17.365      10.096\n",
      "x23           12.0763      7.101      1.701      0.089      -1.841      25.993\n",
      "x24          -22.3418      7.846     -2.848      0.004     -37.719      -6.964\n",
      "x25           16.8570      7.271      2.318      0.020       2.605      31.109\n",
      "x26           -7.2673      7.177     -1.013      0.311     -21.333       6.799\n",
      "x27            3.0030      6.679      0.450      0.653     -10.087      16.093\n",
      "x28           -5.1571      6.221     -0.829      0.407     -17.350       7.036\n",
      "x29           11.2090      5.630      1.991      0.046       0.174      22.244\n",
      "x30          -25.9290      7.156     -3.623      0.000     -39.954     -11.904\n",
      "x31           35.3632      8.552      4.135      0.000      18.601      52.126\n",
      "x32          -16.1620      6.126     -2.638      0.008     -28.169      -4.155\n",
      "x33          -11.3750      6.817     -1.669      0.095     -24.735       1.985\n",
      "x34           21.1145      8.467      2.494      0.013       4.520      37.709\n",
      "x35          -15.8914      7.799     -2.038      0.042     -31.177      -0.606\n",
      "x36           13.4705      7.002      1.924      0.054      -0.253      27.194\n",
      "x37           -9.3884      6.060     -1.549      0.121     -21.265       2.489\n",
      "x38           18.1735      7.592      2.394      0.017       3.294      33.053\n",
      "x39          -10.6576      6.740     -1.581      0.114     -23.868       2.553\n",
      "x40           -4.3662      4.567     -0.956      0.339     -13.318       4.586\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.11 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "###ANSWER\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "Dt = np.array(sonar.values)\n",
    "X = Dt[:,0:40]\n",
    "y = Dt[:,-1].reshape(-1, 1)\n",
    "y[y==-1] = 0\n",
    "\n",
    "Xc = sm.add_constant(X)\n",
    "train_vectors, test_vectors, train_labels, test_labels = \\\n",
    "    train_test_split(Xc, y, test_size=0.15)\n",
    "\n",
    "logit_model = sm.Logit(train_labels, train_vectors)\n",
    "result = logit_model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# print(train_vectors.shape)\n",
    "# print(test_vectors.shape)\n",
    "\n",
    "predicted = result.predict(test_vectors)\n",
    "# print(predicted)\n",
    "\n",
    "nominal = [1.0 if x >0.5 else 0.0 for x in predicted]\n",
    "print(nominal)\n",
    "\n",
    "# print(train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe8700-a3da-42e1-9c24-8e57bcf2c632",
   "metadata": {},
   "source": [
    "Train your percentron class with the training set. \n",
    "* Don't forget that the labels in the sonar dataset is '-1'. You probably need to convert '0' in the train_labels and test_labels back to '-1'.\n",
    "* Use the features in the test set in your percetron prediction function to predict the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817bb68-b9c9-4cfe-8ebf-0ae48b7c7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bdbffc12-3b71-4f2d-a1a0-62ec36aafd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41,)\n",
      "Accuracy:  0.6534090909090908\n",
      "Weights:  [-0.29793  -0.948084 -0.133968 -0.577588 -0.173298 -0.335782  0.692082\n",
      "  0.357196 -0.349576 -0.024684 -0.632716 -0.208682 -0.131192  0.267488\n",
      " -0.198306  0.409656 -0.18591   0.044558 -0.121424  0.055342 -0.121246\n",
      " -0.104304  0.073956 -0.245394  0.12747   0.025704  0.061686 -0.016418\n",
      " -0.0822   -0.426444  0.78677  -0.36891  -0.347412  0.599896 -0.375664\n",
      "  0.290424 -0.072482  0.276816 -0.203708 -0.17511 ]\n",
      "Bias weight: 0.1399999999999994\n",
      "[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "###ANSWER\n",
    "\n",
    "X_ = np.array(train_vectors)\n",
    "# in the line below, the constant is removed, because percentron will add a bias weight\n",
    "Train_X = X_[:,1:]\n",
    "# print(Train_X.shape)\n",
    "# print(Train_X)\n",
    "\n",
    "temp_y = np.array(train_labels)\n",
    "temp_y[temp_y==0] = -1\n",
    "Train_y = temp_y\n",
    "# print(Train_y)\n",
    "\n",
    "## my perceptron\n",
    "new_X = np.concatenate((Train_X, Train_y), axis=1)\n",
    "# print(new_X.shape)\n",
    "# print(new_X)\n",
    "\n",
    "# train perceptron\n",
    "new_p = MyPerceptron(new_X,new_X.shape[0],0.01)\n",
    "new_p.fit()\n",
    "new_p.errors()\n",
    "\n",
    "# test set\n",
    "X_ = np.array(test_vectors)\n",
    "Test_X = X_[:,1:]\n",
    "# print(Test_X.shape)\n",
    "# print(Test_X.shape[0])\n",
    "# print(Test_X)\n",
    "\n",
    "temp_y = np.array(test_labels)\n",
    "temp_y[temp_y==0] = -1\n",
    "Test_y = temp_y\n",
    "# print(Test_y)\n",
    "\n",
    "# use perceptron to predict the labels\n",
    "pred = []\n",
    "for i in range(0,Test_X.shape[0]):\n",
    "    val = new_p.predict(Test_X[i,:])\n",
    "    pred.append(val)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6b82f-88ad-4b05-a9cf-2f46d6f683b9",
   "metadata": {},
   "source": [
    "---\n",
    "&#9989; **Do This**: Give a short discussion of the comparison between the results from the different classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70602a9",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c5e3b-c1eb-4be2-8da6-3027105e540c",
   "metadata": {},
   "source": [
    "&#169; Copyright 2025, The Department of Computational Mathematics, Science and Engineering; Michigan State University"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
