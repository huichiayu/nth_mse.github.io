{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ANSWER\n",
    "(for making sure this gets removed)\n",
    "\n",
    "### INSTRUCTOR CONTENT -- REMOVE THIS IN STUDENT VERSION\n",
    "#### The goals/notes for this class are:\n",
    "\n",
    "After completing this activity, students will be able to:\n",
    "1. Construct a data set using `make_blobs`\n",
    "1. Fit an SVM model to that data\n",
    "1. Find the optimal line of separation using attributes from the fitted SVM model\n",
    "   - Describe what a \"support vector\" is\n",
    "   - Find the margins of separation\n",
    "2. Draw the found boundary and margins\n",
    "   - Investigate how does changing the penalty value \"C\" affects the process\n",
    "\n",
    "* Before embarking on \"real\" data, we want to create a space for students to think a bit more about how SVMs work, unpack what the fit parameters represent, and how one of the model parameters will influence the results.\n",
    "\n",
    "#### In-class meta-messaging\n",
    "\n",
    "* Highlight that the goal for today is to use some artificial data as a testing ground for making sense of how SVMs work, what the parameters returned by the fit represent, and how the hyperparameter \"C\" in the model can effect the outcomes.\n",
    "* There's a bit of math buried in here when it comes from unpacking how to compute the margin lines, so you might want to give students a heads up about this and encourage them to ask questions of their group and their instructors if they have questions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment: Machine learning with an SVM Classifier\n",
    "# Day 19\n",
    "# CMSE 202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; **Put your name here** </p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Put your group member names here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/7sFL3.png\" width=400px>\n",
    "\n",
    "### Agenda for today's class\n",
    "\n",
    "1. [Review of Pre-Class assignment](#review)\n",
    "2. [The SVM Classifier](#svm)\n",
    "3. [Visualizing the boundary and the margins](#viz)\n",
    "4. [Hyperparameter tuning (the effect of C)](#hyper-tuning)\n",
    "   \n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the day\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"review\"></a>\n",
    "## 1. Review of Pre-Class assignment\n",
    "\n",
    "We'll discussion any questions that came up as a class.\n",
    "\n",
    "#### What does an SVM do? How do you define the \"optimal\" line of separation?\n",
    "\n",
    "&#9989; **Do This:** In the pre-class, we reviewed what a Support Vector Machine (SVM) is and how it worked. We asked you to consider how you might draw an \"optimal\" line of separation of a 2-class problem. Discuss this with your group members now and record the answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do This -  Write your discussion notes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"svm\"></a>\n",
    "## 2. The SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you watched in the pre-class videos, the basic idea of the Support Vector Machine classifier is as follows:\n",
    "\n",
    "* The algorithm creates a line and marks the points from each class that are the \"closest\" to the line. These points are called the **support vectors**. In the image at the top, the black dots have one support vector and the white circles have two (two points equidistant from the line). We then compute the distance between the line and the support vectors. This distance is called the **margin**. \n",
    "\n",
    "* An SVM tries to make a decision boundary, in the form of a line, such that the separation between the two classes is as wide as possible.\n",
    "\n",
    "In this class, we'll try running an SVM classifier and then see if we can judge how well it did. We can compare this to the line you made by hand in the pre-class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classify your \"blob\" data\n",
    "\n",
    "Let's pull your \"blob\" data and graph forward from the pre-class assignment and use it here as data to run an SVM classifier on. We know by inspection that the data is linearly separable and that you tried come up with the equation of a line that separates the two classes.  \n",
    "\n",
    "&#9989; **Do This:** Using what you did in your pre-class assignment, recreate the blob data, your scatter plot and the line you drew to separate the blobs. It should be the case that blobs created with the same `random_state` should make the same blobs. Is that true? **If you didn't get all of this worked out in your pre-class assignment, check in with your group and see what they did!**\n",
    "\n",
    "**It might be a good idea for everyone in your group to use the same `random_state` so you can compare more easily.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make your blobs, scatter plot them and draw your line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "X,y = make_blobs(n_features=2, centers=2, random_state=3)\n",
    "clrs = ['red' if cls == 1 else 'green' for cls in y]\n",
    "plt.scatter(X[:,0], X[:,1], c=clrs)\n",
    "xline = np.linspace(-6,3)\n",
    "yline = xline * -2 -1.5\n",
    "plt.plot(xline,yline);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Running the SVM classifier\n",
    "\n",
    "The basic process for making an SVM classifier, as with all classifiers, is a follows:\n",
    "\n",
    "- create the model\n",
    "- fit the model\n",
    "- examine the result\n",
    "\n",
    "Rather than trying to build an SVM classifier from scratch like you did with the Perceptron model, we're going to use the `sklearn` module. You can find the main scikit-learn documentation [here](https://scikit-learn.org/stable/user_guide.html).\n",
    "\n",
    "To accomplish the steps above, you would use the `sklearn.svm.svc` (short for \"Support Vector Classifier\") to do the work. Here are some of the details.\n",
    "- To build the model you would use `svm.SVC`. This constructor takes a number of parameters:\n",
    "   - you can see all the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">parameters here</a>.\n",
    "   - the most important are the first two:\n",
    "      - **C** this is a parameter of how the fit is to be done. We will explore this later, for now the default is fine.\n",
    "      - **kernel** for linear separability we are going to use the `\"linear\"` kernel. Note that the kernel name is a string.\n",
    "   - The constructor also returns a model\n",
    "- You then call `my_model.fit()` (or whatever you called your model variable) to fit the data. It takes two arguments:\n",
    "   - The array of data. Its shape is (`n_samples`, `n_features`). That is, the provided array is `n_samples` large and each sample (each element in the array) is an array of `n_features`.\n",
    "   - An array of class labels. It should be the size of `n_samples`, one for each data element. Typically these are integers.\n",
    "- This function returns the same model you called `fit()` from (thus you don't need the return value)\n",
    "\n",
    "**Note: we are not using `train_test_split` for this model, you will fit your SVM model to the whole data set. We are simply comparing the linear separation you guessed to the one SVC produces.**\n",
    "\n",
    "&#9989; **Do This:** Create an SVC classifier using the `\"linear\"` kernel and with `C=10` (if you don't specify the value for `C` argument, it will use the default, which is `1.0`). Then, fit the model to your data. **Note**: you should not expect any meaningful output just yet, so as long as your call to `fit()` doesn't throw an error, you should be good to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your classifier and call the fit function on your data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "cls = svm.SVC(kernel=\"linear\", C=10)\n",
    "print(X.shape, y.shape)\n",
    "cls.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Upacking the SVM results\n",
    "\n",
    "What information can we get from the fitted SVM? Remember that these fitted models have [a number of attributes and methods](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) that provide use with information regarding the fit.\n",
    "\n",
    "There are some useful attributes:\n",
    "\n",
    "- The attributes `coef_` and `intercept_` (note the underline) can be used to draw the the boundary the classifier created. We'll talk more about that below.\n",
    "- The attribute `support_vectors_` are the coordinates of the support vectors used by the algorithm. In this case these are the 2D location of the data points that were determined to be the support vectors.\n",
    "\n",
    "And some useful methods:\n",
    "\n",
    "- `predict` takes single parameter, an array of shape (`number_of_data_to_predict`, `number_of_features`) and returns an array of class predictions.\n",
    "- `decision_function` takes single parameter, an array of shape (`number_of_data_to_predict`, `number_of_features`) and returns the distance from the decision border for each datum. The sign indicates the class the datum belongs to.\n",
    "- `score`, takes two parameters: array sample data of shape (`number_of_data_to_predict`, `number_of_features`) and array of size `number_of_data_to_predict` class labels and returns a percentage correct.\n",
    "\n",
    "&#9989; **Do This:**  Explore some of the methods above and write a bit of code that prints the accuracy of our formed classifier. What function should you use for this? Check the accuracy using the same data you used for find the model fit -- does the result match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore some of the functions from above and print the accuracy of your model for the data you provided above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "print(f\"Accuracy: {cls.score(X,y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"viz\"></a>\n",
    "## 3. Visualizing the boundary and the margins\n",
    "\n",
    "We won't go through the nitty-gritty details for \"how\" an SVM algorithm defines the optimal boundary -- the pre-class video described conceptually what is happening -- but we are going to try and get a few terms straight so we can plot some results and compare to our prediction.\n",
    "\n",
    "### 3.1 What is a hyperplane?\n",
    "\n",
    "A <a href=\"https://en.wikipedia.org/wiki/Hyperplane\"> hyperplane </a> is geometric element whose dimensionality is one less than the space in which it is embedded. For our purposes, a hyperplane divides the embedded space. Thus:\n",
    "- a 2D space can be divided by a 1D line\n",
    "- a 3D space can be divided by a 2D plane\n",
    "- and so on...\n",
    "   \n",
    "The data that we generated is in 2D and thus we are trying to find a 1D hyperplane -- a line -- to divide the space. In general, an SVM can search in any n-dimensional space and find a n-1 hyperplane (or at least find the best one it is capable of) that divides the space. SVM literature thus talks always about hyperplanes as a more general concept. For the moment we will be content with hyperplanes in 1D, **which are lines**.\n",
    "\n",
    "### 3.2 Drawing the boundary\n",
    "\n",
    "We are going to use the attributes of the fitted SVM to draw the line that was determined by the mode. First, let's remember that the equation of a line in slope-intercept format is:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Where $m$ is the slope and $b$ is the intercept. However, there is another form of a line equation that is called the \"standard\" form which is:\n",
    "\n",
    "$$ Ax + By + C = 0 $$\n",
    "\n",
    "where $A$ and $B$ are the **coefficients** of the line. You should be able to convert back and forth between these two forms relatively easily. We had to do this when thinking about how to draw the best fit line from the Perceptron model as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These standard coefficients are what are returned by the `.coef_` attributes of the classifier hyperplane. Assuming that we are doing a 2D (2 feature) classifier, the coefficients represent a line in standard format. Thus:\n",
    "\n",
    "$$ Ax + By + C = 0 $$\n",
    "\n",
    "then `.coef_` returns an array of $[A, B]$ and `intercept_` returns the $C$ value of the line.\n",
    "\n",
    "By pulling that information from the model, you should be able to determine the slope-intercept form and draw the decision boundary through the \"blobs\". \n",
    "\n",
    "&#9989; **Do This:** Plot the blobs again along with your predicted line from the pre-class assignment **as well as the line from the model (i.e., the decision boundary)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to draw the blobs and the resulting decision boundary line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "# Create the hyperplane\n",
    "w = cls.coef_[0]\n",
    "slope = -w[0] / w[1]\n",
    "intercept = (cls.intercept_[0]) / w[1]\n",
    "xx = np.linspace(-6, 2.5)\n",
    "yy = slope * xx - intercept\n",
    "\n",
    "# Plot the hyperplane\n",
    "plt.plot(xx, yy, 'r--')\n",
    "\n",
    "## Plot the guess we made\n",
    "xline = np.linspace(-6,3)\n",
    "yline = xline * -2 -1.5\n",
    "plt.plot(xline,yline, 'b-')\n",
    "\n",
    "plt.legend(['Decision Boundary', 'Our Guess'])\n",
    "\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=clrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question:** How well do your guess and the decision boundary agree? Is the decision boundary rotated at all when compared to your guess? If so, why would it be rotated that way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do this - Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do This:** Print the slope and the intercept you have calculated. Three decimal points of accuracy will do. **Confirm that these numbers match the line representing the decision boundary in your plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the slope and intercept here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "print(f\"Slope:{slope:.3}, Intercept:{intercept:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Draw the Support Vectors\n",
    "Remember that the attribute `support_vectors_` (note the trailing underline) of the model yields a list of points that are used as support vectors. \n",
    "\n",
    "&#9989; **Do This:** Using `support_vectors_`, redraw the blobs and boundary but now also mark the support vectors the algorithm used to determine the boundary. If you have trouble-coming up with a good way of marking the support vectors, chat with your group to brainstorm ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw now with support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "plt.plot(xx, yy)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=clrs)\n",
    "\n",
    "sv = cls.support_vectors_\n",
    "plt.scatter(sv[:, 0], sv[:, 1], s=200,\n",
    "           linewidth=1, facecolors='none', edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 SVM Margins\n",
    "\n",
    "Besides drawing the boundary, the SVM tries to draw a **margin**, a set of parallel lines on either side of the boundary. You learned about this in the pre-class video.\n",
    "\n",
    "This width of this margin is controlled by the `C` parameter. When you create the model (the `C` parameter is always positive), `C` tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of `C`, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of `C` will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of `C`, you should get misclassified examples, often even if your training data is linearly separable. \n",
    "\n",
    "* Large `C` - better job of classifying training data; smaller margin\n",
    "* Smaller `C` - more misclassifcations; larger margin\n",
    "\n",
    "#### 3.4.1 Calculating the margin\n",
    "\n",
    "Below, we have written a function that can help you draw the margins so that you can see the effect that `C` has on the SVM results. To think about how this works, let's look at the image from the top of the notebook again:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/7sFL3.png\" width=300px>\n",
    "\n",
    "The way that margins are determined follows this procedure:\n",
    "\n",
    "- Looking at the image, the distance between the two margins is always $\\frac{2}{\\| w \\|}$, and the distance from the boundary to one margin half that value (the margins are equidistant from the boundary)\n",
    "   - $w$ is the vector $[A,B]$ (the coefficients of x and y in standard form), so $\\| w \\| = \\sqrt{A^2 + B^2} $ and the margin distance is therefore $\\frac{1}{\\sqrt{A^2 + B^2}} $\n",
    "- Given the boundary line x and y arrays, the updated $y_{margin-down}$ array is: $ y - \\sqrt{1 + m^2} $ and $y_{margin-up}$ array is: $y + \\sqrt{1 + m^2} $\n",
    "\n",
    "#### 3.4.2 Describing the function `margin_y_fn`\n",
    "\n",
    "The function below, `margin_y_fn`, takes 3 arguments:\n",
    "- the y array of values of the boundary line\n",
    "- The `A` coefficient from the fit\n",
    "- The `B` coefficient from the fit\n",
    "\n",
    "It returns two values:\n",
    "- an array of y values for the lower margin (given the same x values as the boundary line)\n",
    "- an array of y values for the upper margin (also given the same x values as the boundary line).\n",
    "\n",
    "The function appears below. Make sure to run it before trying the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_y_fn(y, A, B):\n",
    "    slope = -A/B\n",
    "    margin_dist = 1/np.sqrt(A**2 + B**2)\n",
    "    y_down = y - np.sqrt(1 + slope** 2) * margin_dist\n",
    "    y_up = y + np.sqrt(1 +  slope** 2) * margin_dist\n",
    "    return y_down, y_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do This:**  Using the function `margin_y_fn`, make a plot with: the blobs, the boundary, the support vectors, and, now, the margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "# margin = 1 / np.sqrt(np.sum(cls.coef_ ** 2))\n",
    "# yy_down = yy - np.sqrt(1 + slope ** 2) * margin\n",
    "# yy_up = yy + np.sqrt(1 + slope ** 2) * margin\n",
    "\n",
    "yy_down, yy_up = margin_y_fn(yy, cls.coef_[0][0], cls.coef_[0][1])\n",
    "\n",
    "plt.plot(xx, yy)\n",
    "plt.scatter(X[:,0], X[:,1], c=clrs)\n",
    "plt.scatter(sv[:, 0], sv[:, 1], s=200,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"hyper-tuning\"></a>\n",
    "## 4. Hyperparameter tuning (the effect of `C`)\n",
    "\n",
    "As we discussed the parameter `C` controls that amount of misclassification that is acceptable:\n",
    "\n",
    "* Large `C` - better job of classifying training data; smaller margin\n",
    "* Smaller `C` - more misclassifcations; larger margin\n",
    "\n",
    "In this last section, we will look into the effect of choosing a different value of `C` for the same data. We will pull together all the work you have done so far into a single cell. \n",
    "\n",
    "&#9989; **Do This:** Below, copy and paste all the necesssary code that you wrote above such that the cell below does the following:\n",
    "\n",
    "* make your fake data\n",
    "* create an SVC model and fit it to your data\n",
    "* calculate the decision boundaries\n",
    "* plot the data, the optimal line, the decision boundaries and the support vectors, and\n",
    "* print the accuracy.\n",
    "\n",
    "Make sure you run the cell and confirm that it produces the output you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "# Create the data\n",
    "X,y = make_blobs(n_features=2, centers=2, random_state=4)\n",
    "clrs = ['red' if cls == 1 else 'green' for cls in y]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:,0], X[:,1], c=clrs)\n",
    "\n",
    "# Create the fit\n",
    "cls = svm.SVC(kernel=\"linear\", C=10)\n",
    "cls.fit(X,y)\n",
    "\n",
    "# Create the hyperplane\n",
    "w = cls.coef_[0]\n",
    "slope = -w[0] / w[1]\n",
    "intercept = (cls.intercept_[0]) / w[1]\n",
    "xx = np.linspace(X[:,0].min(), X[:,0].max())\n",
    "yy = slope * xx - intercept\n",
    "\n",
    "# Plot the hyperplane\n",
    "plt.plot(xx, yy, 'r--')\n",
    "\n",
    "# Mark the support vectors\n",
    "sv = cls.support_vectors_\n",
    "plt.scatter(sv[:, 0], sv[:, 1], s=200,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "# Plot the margins\n",
    "yy_down, yy_up = margin_y_fn(yy, cls.coef_[0][0], cls.coef_[0][1])\n",
    "plt.plot(xx, yy_down, 'k--')\n",
    "plt.plot(xx, yy_up, 'k-')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy: {cls.score(X,y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Automate the process to explore the effect of \"C\"\n",
    "\n",
    "Now that you have pulled at that code together and checked that it works, put it inside a function called `test_C_parameter` that runs all that code and takes the following arguments:\n",
    "* `C` - the hyperparamter that controls misclassifications\n",
    "* `random_state` - the state associated with the created data\n",
    "\n",
    "Then run the function using a variety of choices for `C` to observe how the margins change. You might need to change `random_state` to get distributions of blobs that overlap a bit more in order to really understand how changing `C` influences the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put any an all code you need here, create additional cells as necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "def test_C_parameter(C=10, random_state=3):\n",
    "\n",
    "    # Create the data\n",
    "    X,y = make_blobs(n_features=2, centers=2, random_state=random_state)\n",
    "    clrs = ['red' if cls == 1 else 'green' for cls in y]\n",
    "\n",
    "    # Plot the data\n",
    "    plt.scatter(X[:,0], X[:,1], c=clrs)\n",
    "\n",
    "    # Create the fit\n",
    "    cls = svm.SVC(kernel=\"linear\", C=C)\n",
    "    cls.fit(X,y)\n",
    "\n",
    "    # Create the hyperplane\n",
    "    w = cls.coef_[0]\n",
    "    slope = -w[0] / w[1]\n",
    "    intercept = (cls.intercept_[0]) / w[1]\n",
    "    xx = np.linspace(X[:,0].min(), X[:,0].max())\n",
    "    yy = slope * xx - intercept\n",
    "\n",
    "    # Plot the hyperplane\n",
    "    plt.plot(xx, yy, 'r--')\n",
    "\n",
    "    # Mark the support vectors\n",
    "    sv = cls.support_vectors_\n",
    "    plt.scatter(sv[:, 0], sv[:, 1], s=200,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "    # Plot the margins\n",
    "    yy_down, yy_up = margin_y_fn(yy, cls.coef_[0][0], cls.coef_[0][1])\n",
    "    plt.plot(xx, yy_down, 'k--')\n",
    "    plt.plot(xx, yy_up, 'k-')\n",
    "    plt.title('C = '+str(C))\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Accuracy: {cls.score(X,y)}\")\n",
    "    \n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "\n",
    "test_C_parameter(C=10, random_state=4)\n",
    "test_C_parameter(C=1, random_state=4)\n",
    "test_C_parameter(C=0.1, random_state=4)\n",
    "test_C_parameter(C=0.01, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question:** What do you notice about changing `C` in terms of the properties of the margins and the misclassifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Do this - Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Congratulations, we're done!\n",
    "\n",
    "Now, you just need to submit this assignment by uploading it to the course <a href=\"https://d2l.msu.edu/\">Desire2Learn</a> web page for today's submission folder (Don't forget to add your names in the first cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2023 Department of Computational Mathematics, Science and Engineering at Michigan State University"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
