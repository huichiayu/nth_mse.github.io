{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final exam (Individual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Put your name here.</p>\n",
    "### <p style=\"text-align: right;\"> &#9989; Put your _GitHub username_ here.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 202 Midterm (Section 003 - Spring 25)\n",
    "\n",
    "\n",
    "# &#128721; READ EVERYTHING CAREFULLY\n",
    "\n",
    "\n",
    "The goal of this midterm is to give you the opportunity to test out some of the skills that you've developed thus far this semester. In particular, you'll practice setting up a GitHub repository, committing and pushing repository changes, downloading data with command line tools, performing some data analysis, possibly using a new Python package, and writing a python class. You should find that you have all of the skills necessary to complete this exam with even just eight weeks of CMSE 202 under your belt!\n",
    "\n",
    "You are encouraged to look through the entire exam before you get started so that you can appropriately budget your time and understand the broad goals of the exam. Once you've read through it, try doing Parts 1 and 2 first so that you have your repository set up and you download all necessary data files as they will be necessary to complete the assigned tasks. Let your instructor know right away if you have problems downloading the data!\n",
    "\n",
    "The exam is set up so that even if you get stuck on one part there are opportunities to get points on the other parts, so consider jumping ahead if you feel like you aren't making progress and then come back later if you have time.\n",
    "\n",
    "**Important note about using online resources**: This exam is \"open internet\". That means that you can look up documentation, google how to accomplish certain Python tasks, etc. Being able to effectively use the internet for computational modeling and data science is a very important skill, so we want to make sure you have the opportunity to exercise that skill. **However**: The use of any person-to-person communication software is absolutely not acceptable. If you are seen accessing your email, using a chat program (e.g. Slack), or any sort of collaborative cloud storage or document software (e.g. Google Documents), you will be at risk for receiving a zero on the exam.\n",
    "\n",
    "**Important Guidelines on AI Tool Usage**\n",
    "\n",
    "This exam allows the use of AI tools (such as ChatGPT, Claude, HiTA) with specific guidelines that mirror real-world professional practices. These tools should enhance your learning and problem-solving process, not replace your intellectual engagement. Here are the key requirements:\n",
    "\n",
    "1. **Appropriate Use of AI**:\n",
    "   - Use AI as a learning assistant to understand concepts, debug code, or get unstuck\n",
    "   - Use AI to improve your solution approach or verify your thinking\n",
    "   - Use AI to learn about new Python packages or functions you might need\n",
    "\n",
    "2. **Prohibited Uses**:\n",
    "   - Direct copying of AI-generated solutions without understanding\n",
    "   - Asking AI to complete entire questions without your intellectual input\n",
    "   - Using AI to modify provided starter code or test cases\n",
    "   - Using AI to circumvent learning objectives or problem requirements\n",
    "\n",
    "3. **Documentation Requirements**:\n",
    "   - You must cite any AI assistance received by adding a comment that includes:\n",
    "     * The AI tool used\n",
    "     * The specific question you asked\n",
    "     * How you modified or improved upon the AI's suggestion\n",
    "   - Example: \"# AI assistance: Used Claude to understand lmfit parameter initialization. Modified the suggested approach to include custom bounds.\"\n",
    "\n",
    "4. **Evaluation Implications**:\n",
    "   - Questions showing signs of direct AI solution copying will receive zero points\n",
    "   - Evidence of not following problem instructions, even if AI-suggested, will result in zero points\n",
    "   - Modified starter code or test cases will result in zero points for that question\n",
    "\n",
    "Remember: The goal is to demonstrate your understanding and problem-solving abilities. AI tools should support your learning, not replace your critical thinking and coding skills.\n",
    "\n",
    "**Note**: Traditional open internet resources (documentation, Stack Overflow, etc.) remain available, but person-to-person communication tools are not permitted.\"\n",
    "\n",
    "**Keep your eyes on your screen!** Unfortunately, there isn't enough space in the room for everyone to sit at their own table so please do your best to keep your eyes on your own screen. This exam is designed to give *you* the opportunity to show the instructor what you can do and you should hold yourself accountable for maintaining a high level of academic integrity. If any of the instructors observe suspicious behavior, you will, again, risk receiving a zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "\n",
    "0. [Part 0: Upgrade packages](#part0) (1 point)\n",
    "\n",
    "1. [Part 1: Git](#part1) (9 points)\n",
    "\n",
    "2. [Part 2: Data Preprocessing](#part2) (20 points)\n",
    "\n",
    "3. [Part 3: Regression](#part3) (15 points)\n",
    "\n",
    "4. [Part 4: Principal Component Analysis](#part4) (12 points)\n",
    "\n",
    "5. [Part 5: Support Vector Machines](#part5) (15 points)\n",
    "\n",
    "6. [Part 6: Regression Redux](#part6) (7 points)\n",
    "\n",
    "7. [Part 7: Conceptual Questions](#part7) (14 points)\n",
    "\n",
    "8. [Part 8: Conclusion](#conclusion) (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:50:14.352819Z",
     "start_time": "2022-12-07T03:50:14.350105Z"
    }
   },
   "outputs": [],
   "source": [
    "grades = [1, 9, 20, 15, 12, 15, 7, 14, 3]\n",
    "\n",
    "print(f\"The total grade for this final is {sum(grades)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part0\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 0: Upgrade Packages\n",
    "\n",
    "**&#9989; Question 0.1 (1 point)**: Run the cell below. Do you have the correct packages ? If not upgrade them. **You must do this in order to avoid issues in the rest of the notebook.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:15:16.292936Z",
     "start_time": "2022-12-07T03:15:16.283043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version should be 1.1.3 and I have 1.6.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Standard libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Data cleaning Libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.utils import resample\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn import __version__ as sk_version\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "print(f\"Sklearn version should be 1.1.3 and I have {sk_version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part1\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "## Part 1: Git (9 points)\n",
    "\n",
    "For this assignment, you're going to add it to the `cmse202-s25-turnin` repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Question 1.1 (1 point)**: Navigate to your `cmse202-s25-turnin` **local** repository and create a new directory called `final` and copy this notebook in that new directory.\n",
    "\n",
    "``` bash\n",
    "# Put the command(s) for creating the new directory\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.2 (3 points)** Check the status of your local `git`.\n",
    "\n",
    "``` bash \n",
    "# Put the command you used to check the status of git\n",
    "\n",
    "```\n",
    "Copy and paste below the output of the command.\n",
    "\n",
    "``` bash\n",
    "# Paste it here\n",
    "```\n",
    "\n",
    "What is the name of the branch you are in ? \n",
    "\n",
    "``` bash\n",
    "# Put your answer here\n",
    "```\n",
    "\n",
    "**Important:** You should be in the `main` branch. If you are not switch to the `main` branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.3 (3 points):**\n",
    "Add your name and GitHub username to the top of the notebook, then add and commit **ONLY** the notebook.\n",
    "\n",
    "``` bash\n",
    "# Put the command(s) to add and commit here \n",
    "```\n",
    "\n",
    "What is the commit message you used ?\n",
    "\n",
    "``` bash\n",
    "# Answer the question here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.4 (1 point):** Before moving on. Check that the notebook you are working on is the correct one. Run the following cell. **Are you in the new folder you just created?** If not close this notebook and open the one in the `final` folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T20:23:32.165597Z",
     "start_time": "2022-12-05T20:23:31.986031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luciano/Documents/Teaching/Spring2025/CMSE202/cmse202-S25-jb/exams/final/FS22\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.5 (1 point):** Finally push the updated notebook to GitHub.\n",
    "\n",
    "``` bash\n",
    "# Put the command you used to push to GitHub here.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" repository (you should have done this in the previous homework assignment).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the notebook, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-f22-turnin`\" repository inside the `final` directory that you just created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "<a id=\"part2\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 2. Data preprocessing (19 points)\n",
    "\n",
    "For this assignment we’re going to be working with a dataset that contains measurements of the characteristics of multiple wines. This includes things like the acidity and density. You’ll be asked to use this information, along with the machine learning tools that we’ve used in class, to determine whether the wine type is Red or White.\n",
    "\n",
    "The dataset is located at:\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-F22-data/main/data/winequality.csv`\n",
    "\n",
    "\n",
    "**&#9989; Question 2.1 (4 points):** Do this\n",
    "\n",
    "1. Download the data and type the command you used to download the data\n",
    "\n",
    "2. Read the `winequality.csv` file into a dataframe\n",
    "\n",
    "3. Print out the **unique** labels in the `type` column. \n",
    "\n",
    "4. Display the first 10 rows of the dataset.\n",
    "\n",
    "**Note**: each row represents one data point and each column (except the `type` column) represents one feature. The `type` column corresponds to the class labels for every data point. There are two types of unique class labels in the `type` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T20:23:35.697903Z",
     "start_time": "2022-12-05T20:23:35.694510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put the command you used to download the data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:18:45.842670Z",
     "start_time": "2022-12-07T03:18:40.299370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  381k  100  381k    0     0  1133k      0 --:--:-- --:--:-- --:--:-- 1134k\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "!curl -O https://raw.githubusercontent.com/msu-cmse-courses/cmse202-F22-data/main/data/winequality.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T20:23:36.383889Z",
     "start_time": "2022-12-05T20:23:36.381940Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:12.258632Z",
     "start_time": "2022-12-07T03:22:12.100605Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white' 'red']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fixed acidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "volatile acidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "citric acid",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "residual sugar",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "chlorides",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "free sulfur dioxide",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total sulfur dioxide",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "density",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sulphates",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "alcohol",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "quality",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "03d6aba9-a60c-48c3-8966-a0a97633c42c",
       "rows": [
        [
         "0",
         "white",
         "7.0",
         "0.27",
         "0.36",
         "20.7",
         "0.045",
         "45.0",
         "170.0",
         "1.001",
         "3.0",
         "0.45",
         "8.8",
         "6"
        ],
        [
         "1",
         "white",
         "6.3",
         "0.3",
         "0.34",
         "1.6",
         "0.049",
         "14.0",
         "132.0",
         "0.994",
         "3.3",
         "0.49",
         "9.5",
         "6"
        ],
        [
         "2",
         "white",
         "8.1",
         "0.28",
         "0.4",
         "6.9",
         "0.05",
         "30.0",
         "97.0",
         "0.9951",
         "3.26",
         "0.44",
         "10.1",
         "6"
        ],
        [
         "3",
         "white",
         "7.2",
         "0.23",
         "0.32",
         "8.5",
         "0.058",
         "47.0",
         "186.0",
         "0.9956",
         "3.19",
         "0.4",
         "9.9",
         "6"
        ],
        [
         "4",
         "white",
         "7.2",
         "0.23",
         "0.32",
         "8.5",
         "0.058",
         "47.0",
         "186.0",
         "0.9956",
         "3.19",
         "0.4",
         "9.9",
         "6"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>white</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0  white            7.0              0.27         0.36            20.7   \n",
       "1  white            6.3              0.30         0.34             1.6   \n",
       "2  white            8.1              0.28         0.40             6.9   \n",
       "3  white            7.2              0.23         0.32             8.5   \n",
       "4  white            7.2              0.23         0.32             8.5   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.045                 45.0                 170.0   1.0010  3.00   \n",
       "1      0.049                 14.0                 132.0   0.9940  3.30   \n",
       "2      0.050                 30.0                  97.0   0.9951  3.26   \n",
       "3      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "4      0.058                 47.0                 186.0   0.9956  3.19   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.45      8.8        6  \n",
       "1       0.49      9.5        6  \n",
       "2       0.44     10.1        6  \n",
       "3       0.40      9.9        6  \n",
       "4       0.40      9.9        6  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "# 1 point for downloading the dataset\n",
    "# 1 point for reading the data\n",
    "df = pd.read_csv('winequality.csv')\n",
    "# 1 point for printing the unique labels\n",
    "print(df['type'].unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**&#9989; Question 2.2 (7 points):** Do the following:\n",
    "\n",
    "1. Drop the `NaN` in the dataset\n",
    "\n",
    "2. Drop the `quality` column\n",
    "\n",
    "2. Replace all of the strings in your `type` column with integers based on the following:\n",
    "\n",
    "    | original type | integer type |\n",
    "    | -------- | -------- |\n",
    "    | white | 0 |\n",
    "    | red | 1 |\n",
    "\n",
    "3. Once you've replaced the labels, display your DataFrame and confirm that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:13.098444Z",
     "start_time": "2022-12-07T03:22:13.095245Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:13.455900Z",
     "start_time": "2022-12-07T03:22:13.427927Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['quality'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1 point for dropping the column\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1 point for saving the new DF\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1 point for replacing the values. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1 point for using inplace or saving the new dataframe\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/envs/cmse802/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cmse802/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/cmse802/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cmse802/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['quality'] not found in axis\""
     ]
    }
   ],
   "source": [
    "### ANSWER \n",
    "# 1 point for dropping the NaN\n",
    "# 1 point for saving the new DF\n",
    "df.dropna(inplace=True)\n",
    "# 1 point for dropping the column\n",
    "# 1 point for saving the new DF\n",
    "df.drop(columns=\"quality\", inplace=True)\n",
    "# 1 point for replacing the values. \n",
    "# 1 point for using inplace or saving the new dataframe\n",
    "df[\"type\"] = df[\"type\"].replace({\"red\": 0, \"white\": 1})\n",
    "# 1 point for printing the new dataset\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**&#9989; Question 2.3 (2 points):** As we've seen, when working with `scikit-learn` it can be much easier to work with the data if we have separate variables: one that stores the feature matrix and one that stores the class labels.\n",
    "\n",
    "**Do This:** Split your DataFrame so that you have two separate DataFrames: (1) one called `features`, which contains all columns of features; and (2) one called `labels`, which is a single-column dataframe that contains all of the *new* integer labels you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:14.395392Z",
     "start_time": "2022-12-07T03:22:14.392835Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:14.774505Z",
     "start_time": "2022-12-07T03:22:14.770509Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ANSWER\n",
    "# 1 point for each correct dataframe\n",
    "features = df.drop(columns=['type'])\n",
    "labels = df['type']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.4 (1 points):** Do this:\n",
    "\n",
    "We need to scale the data. This isn't something we've talked very much about in-class, so we don't expect you to know exactly what is happening here. But it will be important for our PCA later on in the assignment. We decided to put it here so you were aware of it's existence. \n",
    "\n",
    "**Run the following bit of code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:15.765193Z",
     "start_time": "2022-12-07T03:22:15.598135Z"
    }
   },
   "outputs": [],
   "source": [
    "# features = pd.DataFrame(PowerTransformer().fit_transform(features), \n",
    "#                             columns=features.columns, \n",
    "#                             index=features.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.5 (3 points):** Do this:\n",
    "\n",
    "1. Split your data into a training and a testing set with a training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. \n",
    "\n",
    "2. Print the lengths to show you have the right number of entries for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:22:16.922235Z",
     "start_time": "2022-12-07T03:22:16.914221Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4847\n",
      "1616\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "# 1 point for using train_test_split, \n",
    "# 1 point for using the correct inputs\n",
    "# 1 point for printing\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=314159)\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository! (2 points)**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 2\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 3. Regression (15 Points)\n",
    "\n",
    "**&#9989; Question 3.1.1 (3 points):**\n",
    "Let's start making some prediction. \n",
    "\n",
    "1. Make a Logistic Regression model using `statsmodels` for predicting the type of wine. \n",
    "\n",
    "2. Print out the summary of your model fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:24:24.355429Z",
     "start_time": "2022-12-07T03:24:24.237499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.34471449e+00 -1.86761667e+00  1.67077095e-01  3.08045349e-01\n",
      "  -1.77485096e+00 -7.89053820e-01  2.94924237e+00 -9.06863837e-15\n",
      "  -1.24040324e+00 -1.20716396e+00 -1.94886658e-01]]\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "#1 for adding the constant, 1 for fitting the model, 1 for printing out the summary.\n",
    "# x_train_add = sm.add_constant(x_train)\n",
    "# x_test_add = sm.add_constant(x_test)\n",
    "# model_log = sm.Logit(y_train,x_train).fit()\n",
    "# print(model_log.summary())\n",
    "\n",
    "# Fit a Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# 1 point for fitting the model\n",
    "# 1 point for printing the coefficients\n",
    "log_model = LogisticRegression(max_iter=1000, )\n",
    "log_model.fit(x_train, y_train )\n",
    "print(log_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.1.2 (2 points):** Further examine the results by  \n",
    "\n",
    "1. printing a classification report\n",
    "2. making a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:26:35.328300Z",
     "start_time": "2022-12-07T03:26:35.326261Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:26:52.937136Z",
     "start_time": "2022-12-07T03:26:52.925572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "  [[ 395    5]\n",
      " [   7 1209]]\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       400\n",
      "           1       1.00      0.99      1.00      1216\n",
      "\n",
      "    accuracy                           0.99      1616\n",
      "   macro avg       0.99      0.99      0.99      1616\n",
      "weighted avg       0.99      0.99      0.99      1616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "#1 point for the classification report, 1 point for the confusion matrix\n",
    "# y_pred_score_log = model_log.predict(x_test_add)\n",
    "\n",
    "y_pred_score_log = log_model.predict_proba(x_test)[:, 1]\n",
    "y_predict_log = (y_pred_score_log>0.5).astype('int')\n",
    "\n",
    "print(\"Confusion matrix: \\n \",confusion_matrix(y_test, y_predict_log) )\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test,y_predict_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.2.1 (3 points):** Make an reduced Logistic Regression model for predicting the type of wine that uses just three parameters (two from the dataset + a constant). Print out the summary of your model fits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:29:00.206070Z",
     "start_time": "2022-12-07T03:29:00.176201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.133346\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                   type   No. Observations:                 4847\n",
      "Model:                          Logit   Df Residuals:                     4844\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 01 May 2025   Pseudo R-squ.:                  0.7610\n",
      "Time:                        16:15:57   Log-Likelihood:                -646.33\n",
      "converged:                       True   LL-Null:                       -2704.8\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                    3.4582      0.131     26.402      0.000       3.201       3.715\n",
      "total sulfur dioxide     3.3177      0.130     25.594      0.000       3.064       3.572\n",
      "volatile acidity        -2.2940      0.108    -21.320      0.000      -2.505      -2.083\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "#1 for adding the constant, 1 for fitting the model, 1 for printing out the summary.\n",
    "x_train_add2 = x_train_add.filter(['const','total sulfur dioxide','volatile acidity'], axis=1)\n",
    "x_test_add2 = x_test_add.filter(['const','total sulfur dioxide','volatile acidity'], axis=1)\n",
    "model_log2 = sm.Logit(y_train,x_train_add2).fit()\n",
    "print(model_log2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.2.2 (2 points):** Same as above, examine the results of your reduced model by printing a classification report and a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:29:01.144438Z",
     "start_time": "2022-12-07T03:29:01.142337Z"
    }
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:29:01.465870Z",
     "start_time": "2022-12-07T03:29:01.456966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "  [[ 363   37]\n",
      " [  31 1185]]\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       400\n",
      "           1       0.97      0.97      0.97      1216\n",
      "\n",
      "    accuracy                           0.96      1616\n",
      "   macro avg       0.95      0.94      0.94      1616\n",
      "weighted avg       0.96      0.96      0.96      1616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "#1 point for the predictions, 1 point for the confusion matrix\n",
    "y_pred_score_log2 = model_log2.predict(x_test_add2)\n",
    "y_predict_log2 = (y_pred_score_log2>0.5).astype('int')\n",
    "\n",
    "print(\"Confusion matrix: \\n \",confusion_matrix(y_test, y_predict_log2))\n",
    "print(\"\\nClassification Report: \\n\", classification_report(y_test,y_predict_log2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 3.3 (3 points):** How did you pick the best parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your reponse here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER ###\n",
    "\n",
    "They should talk about using the highest t-scores or using guess and check. Honestly, either method is fine. Should get full points as long as they don't just say they picked three random parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository! (2 points)**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 3\", and push the changes to GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part4\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 4. Principal Component Analysis (12 points)\n",
    "\n",
    "The full model uses all 12 features to predict the results. In many cases, we might need to see how close we can get with fewer features. Instead of simply removing features, we will use a Principal Component Analysis (PCA) to determine the combined features that contribute the most to the model (through their accounted variance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:30:10.563879Z",
     "start_time": "2022-12-07T03:30:10.561142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scaler Total explained variance:  [0.30496612 0.18394275] 0.4889088724339899\n",
      "Component:  [-2.06195766e-01 -3.66068198e-01  1.82012482e-01  3.16449145e-01\n",
      " -3.34084166e-01  4.46818253e-01  4.84820918e-01 -1.36866836e-16\n",
      " -2.27790039e-01 -2.89452537e-01 -9.07061067e-02]\n",
      "\n",
      "\n",
      "Normalizer Total explained variance:  0.45727749176234445\n",
      "\n",
      " Component:  [-1.88266720e-01 -3.35737545e-01  1.40395823e-01  4.18297110e-01\n",
      " -2.85170670e-01  4.48878827e-01  4.85077507e-01  2.55440573e-17\n",
      " -2.23827353e-01 -2.64345570e-01 -1.38059840e-01]\n",
      "\n",
      "\n",
      "StandardScaler Total explained variance:  0.5165977572004937\n",
      "\n",
      " Component:  [-0.26791373 -0.38131471  0.14805916  0.22693591 -0.40295221  0.41332567\n",
      "  0.44264789 -0.20828535 -0.19426721 -0.31406296  0.02227302]\n",
      "\n",
      "\n",
      "MinMaxScaler Total explained variance:  0.554554452420359\n",
      "\n",
      " Component:  [-0.08445619 -0.36061646  0.11436721  0.64051664 -0.23837412  0.2688891\n",
      "  0.40361839  0.05798881 -0.19654836 -0.21204589 -0.24595262]\n",
      "\n",
      "\n",
      "MaxAbsScaler Total explained variance:  0.5589048986895979\n",
      "\n",
      " Component:  [-0.05339312 -0.35423068  0.08313028  0.7050135  -0.19064292  0.19139432\n",
      "  0.2970058   0.05838936 -0.24410654 -0.23984005 -0.29326711]\n",
      "\n",
      "\n",
      "Robust Total explained variance:  0.509989637502971\n",
      "\n",
      " Component:  [-0.18629674 -0.40320277  0.41804555  0.16956059 -0.33087426  0.38377509\n",
      "  0.4303606  -0.12721168 -0.2597123  -0.27102207 -0.01227084]\n"
     ]
    }
   ],
   "source": [
    "### ANSWER \n",
    "# In case we need a scaler\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "# from sklearn.preprocessing import QuantileTransformer, PowerTransformer\n",
    "scalers = [ Normalizer(), StandardScaler(), MinMaxScaler(), MaxAbsScaler(), RobustScaler(),]# QuantileTransformer(), PowerTransformer()]\n",
    "names = [\"Normalizer\", \"StandardScaler\", \"MinMaxScaler\", \"MaxAbsScaler\", \"Robust\", \"Quantile\", \"Power\"]\n",
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_df = pca.fit_transform(features)\n",
    "print(f\"No Scaler Total explained variance: \", pca.explained_variance_ratio_, pca.explained_variance_ratio_.sum())\n",
    "print('Component: ', pca.components_[0])\n",
    "\n",
    "\n",
    "for n, sc in zip(names, scalers):\n",
    "    features_norm = pd.DataFrame(sc.fit_transform(features), \n",
    "                             columns=features.columns, \n",
    "                             index=features.index)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_df = pca.fit_transform(features_norm)\n",
    "    \n",
    "    print(f\"\\n\\n{n} Total explained variance: \", pca.explained_variance_ratio_.sum())\n",
    "    print('\\n Component: ', pca.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**&#9989; Question 4.1 (5 points):** Run a Principle Component Analysis (PCA)\n",
    "\n",
    "Since we only have 12 features to start with, let's see how well we can do if we try to aggressively reduce the feature count and use only **2** principle components. \n",
    "\n",
    "1. Using `PCA()` and the associated methods, run a principle component analysis on your `features` dataset using only 2 components. \n",
    "\n",
    "2. Transform the dataset into a new dataset and call it `pca_features`. \n",
    "\n",
    "3. Print the `explained_variance_ratio_` and its sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:32:35.543453Z",
     "start_time": "2022-12-07T03:32:35.541284Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:32:35.841936Z",
     "start_time": "2022-12-07T03:32:35.804399Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total explained variance:  [0.30496612 0.18394275] 0.4889088724339899\n"
     ]
    }
   ],
   "source": [
    "### ANSWER ###\n",
    "# 1 point for running PCA\n",
    "# 1 point for using the correct inputs in PCA\n",
    "# 1 point for fitting\n",
    "# 1 point for transforming\n",
    "# 1 point for printing\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "_ = pca.fit(features)\n",
    "\n",
    "pca_features = pca.transform(features)\n",
    "\n",
    "print('Total explained variance: ', pca.explained_variance_ratio_, pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "&#9989; **Question 4.1.1 (2 points):** What is the **total** explained variance ratio captured by the 2 principle components? (just quote the number) How well do you think a model with these many features will perform? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=+3>&#9998;</font> Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ANSWER\n",
    " 1 point for answering \n",
    " 1 point for explaining \n",
    " \n",
    "These 2 components account for roughly 49% of all variance. I expect the classifier should work fairly bad because we've not captured a large fraction of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 4.2 (3 points):** Do the following:\n",
    "\n",
    "1. Split your new features (`pca_features`) and corresponding labels (the labels are the same as before) into a training and a testing set, with the training set representing 75% of your data. For reproducibility, set the `random_state` argument to `314159`. \n",
    "\n",
    "2. Print the lengths to show you have the right number of entries.\n",
    "\n",
    "Basically do the same you have done in Q2.5 but with the `pca_features` dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:34:12.203423Z",
     "start_time": "2022-12-07T03:34:12.200976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:34:12.470654Z",
     "start_time": "2022-12-07T03:34:12.463269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616, 4847)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ANSWER\n",
    "# 1 point for using train_test_split, \n",
    "# 1 point for using the correct inputs\n",
    "# 1 point for printing\n",
    "pca_x_train, pca_x_test, pca_y_train, pca_y_test = train_test_split(pca_features, \n",
    "                                                                    labels,\n",
    "                                                                    train_size=0.75,\n",
    "                                                                    random_state=314159)\n",
    "len(pca_x_test), len(pca_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository! (2 points)**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 4\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "<a id=\"part5\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 5. Support Vector Machine (15 points)\n",
    "\n",
    "Let's see how an SVM performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**&#9989; Question 5.1 (9 points):** Do the following:\n",
    "\n",
    "1. Build a `SVC` model with a `linear` kernel. (2 points), \n",
    "\n",
    "2. Use `GridSearchCV` to find the best `C` parameter from this list: `'C':[0.0001, 0.001, 0.1, 1, 10]`, (2 points)\n",
    "  \n",
    "3. **Fit** the above model on the training set and **print** the best estimator (2 points)\n",
    "\n",
    "4. Use your best estimator on the test dataset to make prediction. (1 point)\n",
    "\n",
    "5. Print a [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), based on its performance on the testing dataset. (1 point)\n",
    "\n",
    "6. Print the [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py) on the testing dataset  (1 point)\n",
    "\n",
    "**Note:** You can set the `GridSearchCV` keyword argument `n_jobs = -1` to speed things up (this allows the process to run on multiple cores.)\n",
    "\n",
    "**Note:** If `GridSearchCV` is slow you can use the built-in model `LinearSVC` in `sklearn`. This is the same as `SVC(kernel = 'linear')` but it is faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:35:07.832760Z",
     "start_time": "2022-12-07T03:35:07.829838Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:35:15.786518Z",
     "start_time": "2022-12-07T03:35:08.608562Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "parameters = {'C':[0.0001, 0.001, 0.1, 1, 10]}\n",
    "# 1 point for creating an SVM\n",
    "# 1 point for using the correct inputs\n",
    "lin_svm = SVC(kernel = \"linear\") \n",
    "\n",
    "# 1 point for using GridSearch\n",
    "# 1 point for the correct inputs\n",
    "svm = GridSearchCV(lin_svm, param_grid=parameters, n_jobs = -1)\n",
    "\n",
    "# 1 point for fitting with correct inputs\n",
    "svm.fit(pca_x_train, pca_y_train)\n",
    "\n",
    "# 1 point for printing the best\n",
    "print(svm.best_estimator_)\n",
    "\n",
    "# 1 point for predicting on the test dataset\n",
    "y_predict = svm.best_estimator_.predict(pca_x_test) \n",
    "\n",
    "# 1 point for using classification_report on the testing dataset\n",
    "print(classification_report(pca_y_test,y_predict))\n",
    "\n",
    "# 1 point for using confusion_matrix/ConfusionMatrixDisplay with correct inputs\n",
    "# print(confusion_matrix(y_test,y_predict))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(svm.best_estimator_, pca_x_test, pca_y_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 5.1.1 (4 points):** Answer the following questions:\n",
    "\n",
    "1. What is the accuracy score of your model ? **Do not use code** You should be able to read it from the output above!\n",
    "\n",
    "2. How does your SVM model compare with the Regression models? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "\n",
    "Each question gets: 1 point for answering the question, 1 point for answering correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository! (2 points)**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 5\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 6. Regression Redux (7 Points)\n",
    "\n",
    "**&#9989; Question 6.1.1 (3 points):** Let's investigate the Logistic model again and see how it performs on the new PCA-transformed dataset. \n",
    "\n",
    "**Do this**: Make an Logistic model using the PCA features (2 features + constant). You should add a constant to the two PCA features, giving you three total features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:37:09.825758Z",
     "start_time": "2022-12-07T03:37:09.771166Z"
    }
   },
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "#1 for adding the constant, 1 for fitting the model, 1 for printing out the summary.\n",
    "pca_x_train_add = sm.add_constant(pca_x_train)\n",
    "pca_x_test_add = sm.add_constant(pca_x_test)\n",
    "pca_model_log = sm.Logit(pca_y_train,pca_x_train_add).fit()\n",
    "print(pca_model_log.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 6.1.2 (2 points):**  Examine the results by making a confusion matrix (2 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:37:45.505079Z",
     "start_time": "2022-12-07T03:37:45.503036Z"
    }
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T03:37:45.882240Z",
     "start_time": "2022-12-07T03:37:45.874028Z"
    }
   },
   "outputs": [],
   "source": [
    "### ANSWER ###\n",
    "#1 point for the predictions, 1 point for the confusion matrix\n",
    "\n",
    "pca_y_pred_score_log = pca_model_log.predict(pca_x_test_add)\n",
    "pca_y_predict_log = (pca_y_pred_score_log>0.5).astype('int')\n",
    "\n",
    "print(\"Confusion matrix: \\n \",confusion_matrix(pca_y_test, pca_y_predict_log) )\n",
    "print(\"\\nClassification Report: \\n\", classification_report(pca_y_test,pca_y_predict_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository! (2 points)**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 6\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"part7\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 7. Conceptual Questions (14 Points)\n",
    "\n",
    "**&#9989; Question 7.1 (4 points):** Compare your results from 3.2.2 and 6.1.2, two Logistic regression models that used the three most significant features. Which one performed better? Why? Your explanation should discuss the difference between the features in the two models and how that difference might affect how well the model fits the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER ###\n",
    "\n",
    "*+1 point for identifying that the PCA model performed better.*\n",
    "\n",
    "*+3 for the explanation. There's a few ways to answer this. The main point that they should talk about is that the PCA features are, by construction, the most informative features possible. That's why we use PCA; to get composite features that can explain the most variance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 7.2 \n",
    "The following questions probe your understanding of  `recall` and `precision`. You’ll be given a specific scenario, and you will need to decide whether to select a Machine Learning model that maximizes `recall` or `precision`.\n",
    "\n",
    "\n",
    "**&#9989; Question 7.2 (4 points):**  A new disease has been detected in Sweden. Doctors have taken multiple measurements of patients and passed them along to you. You’ve tested various models to predict which patients have the new disease. The Swedish health experts have told you it is critical that they identify **all** of the patients with the new disease so they can put them into quarantine. They tell you that it doesn’t matter if your model accidentally flags some people as having the disease even when they don’t, as it’s much better to tell people who aren’t sick to quarantine than to tell a sick person they don’t need to quarantine.\n",
    "\n",
    "Given this situation, should you choose the model that maximizes `recall` or the model that maximizes `precision`? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ANSWER ###\n",
    "\n",
    "*+2 points for saying that they want to maximize Recall.*\n",
    "\n",
    "*+2 for the explanation. Recall is maximized when you have 0 false negatives, meaning you have successfully identified everyone that has the disease.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**&#9989; Question 7.2.2 (4 points):**\n",
    "You are working for the state of Michigan, helping them with their new unemployment insurance program. You’ve created various models that are meant to detect fraud–i.e., determine whether or not someone applied for unemployment insurance when they shouldn’t have. Your boss tells you that it’s critical that your model doesn’t accidentally accuse an innocent person of fraud. They say it’s fine if your model misses some people who committed fraud as long as there are no false accusations. \n",
    "\n",
    "Given this situation, should you choose the model that maximizes `recall` or the model that maximizes `precision`? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ANSWER ###\n",
    "\n",
    "*+2 points for saying that they want to maximize Precision.*\n",
    "\n",
    "*+2 for the explanation. Precision is maximized when you have 0 false positives, meaning you haven't accidentally falsely accused anyone.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!** (2 points)\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 6\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"conclusion\"></a>\n",
    "[Back to ToC](#toc)\n",
    "\n",
    "# Part 8. Conclusion (3 points)\n",
    "\n",
    "Make sure all of your changes to your repository are committed and pushed to GitHub. \n",
    "Before you leave\n",
    "\n",
    "1. Have you added your name and github username at the top of this notebook? ? (1 point)\n",
    "\n",
    "2. Push the changes to your GitHub repository (1 point)\n",
    "\n",
    "3. Upload your notebook to D2L in case something went wrong with your repository or if you couldn't get the repository to work.  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done! Congrats on finishing your CMSE 202 Midterm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T21:32:42.910515Z",
     "start_time": "2022-12-05T21:32:42.900046Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# FINAL GRADE (Instructor's use only)\",\n",
    "parts = [part_0, part_1, part_2, part_3, part_4, part_5, part_6, part_7]\n",
    "print(f\"Part 0: {part_0}/{grades[0]}\")\n",
    "print(f\"Part 1: {part_1}/{grades[1]}\")\n",
    "print(f\"Part 2: {part_2}/{grades[2]}\")\n",
    "print(f\"Part 3: {part_3}/{grades[3]}\")\n",
    "print(f\"Part 4: {part_4}/{grades[4]}\")\n",
    "print(f\"Part 5: {part_5}/{grades[5]}\")\n",
    "print(f\"Part 6: {part_6}/{grades[6]}\")\n",
    "print(f\"Part 7: {part_7}/{grades[7]}\")\n",
    "\n",
    "total = sum(parts)\n",
    "print(f\"Your final grade is {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2022,  Department of Computational Mathematics, Science and Engineering at Michigan State University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmse802",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
